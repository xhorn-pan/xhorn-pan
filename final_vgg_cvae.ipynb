{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-vgg-cvae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xhorn-pan/xhorn-pan/blob/main/final_vgg_cvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfNT-mlFwxVM"
      },
      "source": [
        "# Convolutional Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f4StLYogtB8"
      },
      "source": [
        "## Env prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-JuIu2N_SQf"
      },
      "source": [
        "!pip uninstall -y tensorflow-gpu tensorflow \n",
        "!pip install -q tf-nightly\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM6bwfcy-IuL"
      },
      "source": [
        "!pip uninstall -y tf-nightly\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WCmPO9xmJs4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6q45POJ4-3n"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IijGIj1bFmmw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8df2218-adce-40fe-b664-3bf2629f4efd"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "%load_ext tensorboard\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#logdir = \"/content/drive/My Drive/Colab Notebooks/vgg-logs\"\n",
        "# logdir = \"./vgg-logs\"\n",
        "# writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxLPonVmndAm"
      },
      "source": [
        "import pathlib\n",
        "files_root = pathlib.Path(\"/content/drive/My Drive/Colab Notebooks/vggface2_test_64x64.zip\")\n",
        "data_dir = \"./ds_vgg\"\n",
        "import zipfile\n",
        "with zipfile.ZipFile(files_root, 'r') as zip_ref:\n",
        "  zip_ref.extractall(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Load the vgg dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iusSgoI-n-3g"
      },
      "source": [
        "data_root = pathlib.Path(data_dir + \"/test\")\n",
        "all_faces_paths = list(data_root.glob(\"*/*\"))#[0::2]\n",
        "#all_faces_paths = all_faces_paths[0::2]\n",
        "all_faces_paths = [str(path) for path in all_faces_paths]\n",
        "\n",
        "\n",
        "train_dataset = [path for i, path in enumerate(all_faces_paths) if i % 10 != 0]\n",
        "test_dataset = [path for i, path in enumerate(all_faces_paths) if i % 10 == 0]\n",
        "\n",
        "trs, ts = len(train_dataset), len(test_dataset)\n",
        "batch_size = 1024\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#load dataset\n",
        "def preprocess_image(path):\n",
        "  image = tf.io.read_file(path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  #image = (tf.cast(image, tf.float32) - 127.5) / 127.5 # normalize to [-1,1] range\n",
        "  image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "  #image[image >= .5] = 1.\n",
        "  #image[image < .5] = 0.\n",
        "  image = tf.math.round(image)\n",
        "\n",
        "  return image\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
        "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE).cache()\n",
        "train_dataset = train_dataset.shuffle(buffer_size=trs).batch(batch_size,\n",
        "                           drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_dataset)\n",
        "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE).cache()\n",
        "test_dataset = test_dataset.shuffle(buffer_size=trs).batch(batch_size,\n",
        "                           drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Wire up the generative and inference network with *tf.keras.Sequential*\n",
        "\n",
        "In our VAE example, we use two small ConvNets for the generative and inference network. Since these neural nets are small, we use `tf.keras.Sequential` to simplify our code. Let $x$ and $z$ denote the observation and latent variable respectively in the following descriptions.\n",
        "\n",
        "### Generative Network\n",
        "This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. $p(x|z)$. Additionally, we use a unit Gaussian prior $p(z)$ for the latent variable.\n",
        "\n",
        "### Inference Network\n",
        "This defines an approximate posterior distribution $q(z|x)$, which takes as input an observation and outputs a set of parameters for the conditional distribution of the latent representation. In this example, we simply model this distribution as a diagonal Gaussian. In this case, the inference network outputs the mean and log-variance parameters of a factorized Gaussian (log-variance instead of the variance directly is for numerical stability).\n",
        "\n",
        "### Reparameterization Trick\n",
        "During optimization, we can sample from $q(z|x)$ by first sampling from a unit Gaussian, and then multiplying by the standard deviation and adding the mean. This ensures the gradients could pass through the sample to the inference network parameters.\n",
        "\n",
        "### Network architecture\n",
        "For the inference network, we use two convolutional layers followed by a fully-connected layer. In the generative network, we mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2pUDANgcOhC"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGLbvBEmjK0a"
      },
      "source": [
        "k_initializer = tf.initializers.TruncatedNormal(stddev=0.02)\n",
        "drop_rate = 0.3\n",
        "class CVAE(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.inference_net = tf.keras.Sequential(\n",
        "      [\n",
        "          tf.keras.layers.InputLayer(input_shape=(64, 64, 3)),\n",
        "          #tf.keras.layers.Conv2D(\n",
        "          #    filters=32, kernel_size=4, strides=(2, 2), activation='relu'),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              filters=64, kernel_size=3, \n",
        "              #strides=(2, 2), \n",
        "              padding=\"SAME\", \n",
        "              #kernel_initializer=k_initializer, \n",
        "              activation='relu'),\n",
        "          #tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              filters=64, kernel_size=3, \n",
        "              #strides=(2, 2), \n",
        "              padding=\"SAME\", \n",
        "              #kernel_initializer=k_initializer, \n",
        "              activation='relu'),\n",
        "          tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              filters=128, kernel_size=3, \n",
        "              #strides=(2, 2), \n",
        "              padding=\"SAME\", \n",
        "              #kernel_initializer=k_initializer, \n",
        "              activation='relu'),\n",
        "          #tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              filters=128, kernel_size=3, \n",
        "              #strides=(2, 2), \n",
        "              padding=\"SAME\", \n",
        "              #kernel_initializer=k_initializer, \n",
        "              activation='relu'),\n",
        "          #tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          # tf.keras.layers.Conv2D(\n",
        "          #     filters=256, kernel_size=3, \n",
        "          #     #strides=(2, 2), \n",
        "          #     padding=\"SAME\", \n",
        "          #     kernel_initializer=k_initializer, activation='relu'),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          # No activation\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    self.generative_net = tf.keras.Sequential(\n",
        "        [\n",
        "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Dense(units=4*4*256, activation=tf.nn.leaky_relu),\n",
        "          tf.keras.layers.Reshape(target_shape=(4, 4, 256)),\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=256,\n",
        "              kernel_size=3,\n",
        "              strides=(2, 2),\n",
        "              padding=\"SAME\",\n",
        "              #kernel_initializer=k_initializer,\n",
        "              activation=tf.nn.leaky_relu),\n",
        "          #tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.UpSampling2D(), # .MaxPooling2D(pool_size=(2, 2)),\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=128,\n",
        "              kernel_size=3,\n",
        "              #strides=(2, 2),\n",
        "              padding=\"SAME\",\n",
        "              #kernel_initializer=k_initializer,\n",
        "              activation=tf.nn.leaky_relu),\n",
        "          #tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=128,\n",
        "              kernel_size=3,\n",
        "              #strides=(2, 2),\n",
        "              padding=\"SAME\",\n",
        "              #kernel_initializer=k_initializer,\n",
        "              #ivation='relu'),\n",
        "              activation=tf.nn.leaky_relu),\n",
        "          #tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.UpSampling2D(), # .MaxPooling2D(pool_size=(2, 2)),\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=64,\n",
        "              kernel_size=3,\n",
        "              #strides=(2, 2),\n",
        "              padding=\"SAME\",\n",
        "              #kernel_initializer=k_initializer,\n",
        "              activation=tf.nn.leaky_relu),\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=64,\n",
        "              kernel_size=3,\n",
        "              #strides=(2, 2),\n",
        "              padding=\"SAME\",\n",
        "              #kernel_initializer=k_initializer,\n",
        "              activation=tf.nn.leaky_relu),\n",
        "          tf.keras.layers.UpSampling2D(), # .MaxPooling2D(pool_size=(2, 2)),\n",
        "          tf.keras.layers.Dropout(drop_rate),\n",
        "          # No activation\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=3, kernel_size=3, strides=(1, 1), padding=\"SAME\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "  @tf.function\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.generative_net(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "\n",
        "    return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-X66l0kXpya"
      },
      "source": [
        "latent_dim = 512\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# keeping the random vector constant for generation (prediction) so\n",
        "# it will be easier to see the improvement.\n",
        "noise = tf.random.normal(shape=[num_examples_to_generate, latent_dim])\n",
        "model = CVAE(latent_dim)\n",
        "model.generative_net.summary()\n",
        "model.inference_net.summary()\n",
        "tf.keras.utils.plot_model(model.generative_net, to_file=\"genertive.png\", show_shapes=True, show_layer_names=False)\n",
        "tf.keras.utils.plot_model(model.inference_net, to_file=\"inference.png\", show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = model.sample(test_input)#.numpy()\n",
        "  #predictions = (predictions + 1.0) / 2.0\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, :])\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  plt.savefig('/content/drive/My Drive/ml_logs/vae_image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNWYl_YHFCyv"
      },
      "source": [
        "generate_and_save_images(model, 0, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss function and the optimizer\n",
        "\n",
        "VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n",
        "\n",
        "$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n",
        "\n",
        "In practice, we optimize the single sample Monte Carlo estimate of this expectation:\n",
        "\n",
        "$$\\log p(x| z) + \\log p(z) - \\log q(z|x),$$\n",
        "where $z$ is sampled from $q(z|x)$.\n",
        "\n",
        "**Note**: we could also analytically compute the KL term, but here we incorporate all three terms in the Monte Carlo estimator for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "    #tf.summary.scalar(\"loss\", loss, description=None)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Training\n",
        "\n",
        "* We start by iterating over the dataset\n",
        "* During each iteration, we pass the image to the encoder to obtain a set of mean and log-variance parameters of the approximate posterior $q(z|x)$\n",
        "* We then apply the *reparameterization trick* to sample from $q(z|x)$\n",
        "* Finally, we pass the reparameterized samples to the decoder to obtain the logits of the generative distribution $p(x|z)$\n",
        "* **Note:** Since we use the dataset loaded by keras with 60k datapoints in the training set and 10k datapoints in the test set, our resulting ELBO on the test set is slightly higher than reported results in the literature which uses dynamic binarization of Larochelle's MNIST.\n",
        "\n",
        "## Generate Images\n",
        "\n",
        "* After training, it is time to generate some images\n",
        "* We start by sampling a set of latent vectors from the unit Gaussian prior distribution $p(z)$\n",
        "* The generator will then convert the latent sample $z$ to logits of the observation, giving a distribution $p(x|z)$\n",
        "* Here we plot the probabilities of Bernoulli distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "source": [
        "#generate_and_save_images(model, 0, random_vector_for_generation)\n",
        "elbo_hist = []\n",
        "loss_hist = []\n",
        "epochs = 100\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for train_x in train_dataset:\n",
        "    print('.', end='')\n",
        "    cag_loss = compute_apply_gradients(model, train_x, optimizer)\n",
        "    loss_hist.append(cag_loss)\n",
        "  print('')\n",
        "  #tf.summary.trace_on(graph=True)\n",
        "  if epoch % 5 == 0:\n",
        "    #with writer.as_default():\n",
        "    #  tf.summary.trace_export(name=\"vae_model\", step=epoch)\n",
        "    #  writer.flush()\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for test_x in test_dataset:\n",
        "      loss(compute_loss(model, test_x))\n",
        "    elbo = -loss.result()\n",
        "    elbo_hist.append(elbo)\n",
        "    #display.clear_output(wait=False)\n",
        "    print('Epoch: {}, Test set ELBO: {}, '\n",
        "          'time elapse for epoch {}'.format(epoch,elbo, time.time() - start_time))\n",
        "    generate_and_save_images(model, epoch, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjFKSd6LBPpS"
      },
      "source": [
        "x = len(loss_hist)\n",
        "#y = len(d_loss_hist)\n",
        "x = np.arange(x)\n",
        "#xg = np.polyfit(x, g_loss_hist, 3)\n",
        "\n",
        "plt.plot(x, loss_hist)\n",
        "plt.savefig('/content/drive/My Drive/ml_logs/vae_loss.png')\n",
        "#plt.plot(x, d_loss_hist)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Kvb1gmBjGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "739d9ece-b5fc-474e-dd29-d99c34838b8a"
      },
      "source": [
        "x = len(elbo_hist)\n",
        "#y = len(d_loss_hist)\n",
        "x = np.arange(x)\n",
        "#xg = np.polyfit(x, g_loss_hist, 3)\n",
        "\n",
        "plt.plot(x, elbo_hist)\n",
        "#plt.plot(x, d_loss_hist)\n",
        "plt.savefig('/content/drive/My Drive/ml_logs/vae_elbo_loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESa/X8Vkth0ji9iVQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}